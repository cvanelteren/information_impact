{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-stability\n",
    "\n",
    "Start initial paper extensions of Rick meta stabililty. \n",
    "\n",
    "## What is metastability?\n",
    "Meta-stability refers to the system being able to switch between stable states. Think of it as energy wells, the wells can be global or local but the system can be stuck in a metastable state for some time. \n",
    "\n",
    "- In the original draft a locally-tree like graph was used. Followed by a degree-based analysis on the probability of flipping of a nodes.  However as will be shown below, meta-stability occurs in lattice graphs as well. This means that the degree approach will not generalize well to other types of graphs. \n",
    "\n",
    "In the original draft metastability was hypothesized to be caused by putting the nodes in a system along a singular axis. On one end of the axis a node was mainly locally correlated with the system, i.e. it mainly processes the noise in the system. On the other end of the spectrum there are nodes that are mainly associated with long-term behavior of the system. \n",
    "\n",
    "## What is the goal of this paper?\n",
    "At its core, we know that the Ising model has a rule that interprets noise and allows for nodal behavior from which noise is combined with correlated behavior. The interaction between noise and this rule allows the system to be driven to one the one hand pattern formation and synchronization, and on the other purely uniform behavior. There is no **mechanism** to be accounted for besides this. Mapping this to some information plane, will provide no further insights. From the master equation it can be hypothesized that noise will be amplified for non-zero temperatures. As the temperature approaches infinity, the noise amplification will be linearized, i.e. the effect of noise will be both higher (y-offset), and linear as a function of input energy for a fixed degree for every node. \n",
    "\n",
    "Doing this mapping for a network with a difference in nodal degree, just alters the possible states a node can be in and whether it is linear or not with noise. However, I feel like this is just a mathematical exercise without providing any novel insights into a property of the kinetic Ising model. \n",
    "\n",
    "Then again writing this (in my opinion trivial result) down could be beneficial as I cannot recall a paper that goes into depth unravelling this behavior. By not pretending it is in any way novel is required and essential. \n",
    "\n",
    "### Conclusions\n",
    "There are some questions regarding the goal of the noise induced dynamical transitions. If the goal is to look for differential contribution by structural differences, then you will find those. However, this will not yield any understanding regarding the tipping points that occur. The tipping points occur due to random influences at a local scale that 'play out' in favor of one of the borders of average magnetization. \n",
    "\n",
    "As a consequence, it is also senseless to ask questions about what properties differentiate these nodes, i.e. what makes the contribution of these nodes different. For example on a lattice, the tipping points also occur. These nodes however have the same structural connectivity. In addition, according to ergodic theorem as we sample into infinity, the information decay curves $I(s_i^{t\\pm\\delta}:S^T)$ where $S^{t}$ is the state distribution at tipping point, will also yield to collapse of the nodal information to be the same over time. \n",
    "\n",
    "On different graph structures one could find some property that yields different information content, but they do not reflect any insights into why the tipping points occur in the Ising model. What is needed is a meaningful tipping point, i.e. a tipping that occurs in the dynamics not caused through stochastic noise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imi.toolbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef13b38d2fe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfcy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfcy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIMI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoolbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfcy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imi.toolbox'"
     ]
    }
   ],
   "source": [
    "from plexsim.models import *\n",
    "import src.toolbox\n",
    "\n",
    "from imi.toolbox import infcy\n",
    "print(imi.toolbox.infcy)\n",
    "from IMI.toolbox import infcy\n",
    "# from information_impact import Utils\n",
    "from information_impact.Utils.graph import recursive_tree\n",
    "import plotly.graph_objects as go, numpy as np, networkx as nx\n",
    "from scipy import optimize, interpolate, ndimage\n",
    "n = 30\n",
    "# g = nx.grid_graph([n, n], True)\n",
    "# g = nx.path_graph(n)\n",
    "# g = nx.cycle_graph(n)\n",
    "g = recursive_tree(5)\n",
    "m = Potts(g, sampleSize = 1)\n",
    "\n",
    "temps = np.linspace(0, 4, 100)\n",
    "sim = m.magnetize(temps, n = int(1e5))\n",
    "sim[0] = ndimage.gaussian_filter1d(sim[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "def sig(x, a, b, c):\n",
    "    return a / (1 + b*np.exp(c * x))\n",
    "fit, _ = optimize.curve_fit(sig, temps, sim[0])\n",
    "x      = np.linspace(0, 5)\n",
    "mini = lambda x, fit, theta: abs(sig(x, *fit) - theta)\n",
    "theta = .5\n",
    "a = optimize.fmin(mini, 1, args = (fit, theta))[0]\n",
    "print(a)\n",
    "line = go.Scatter(x = temps, y = sim[0])\n",
    "spline = go.Scatter(x = x, y = sig(x, *fit))\n",
    "fig = go.Figure(data = [line, spline])\n",
    "fig.add_shape(\\\n",
    "             type = 'line',\\\n",
    "             x0   = a,\\\n",
    "             x1   = a,\\\n",
    "             y0   = 0,\\\n",
    "             y1   = 1,\\\n",
    "             line = dict(color = \"black\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to T optimized point and simulate\n",
    "m.t = a\n",
    "m.reset()\n",
    "# m.states = 1\n",
    "# m.states[0] = 1\n",
    "# target = '(0, 0)'\n",
    "# for idx, i in enumerate(nx.shortest_path(m.graph, target)):\n",
    "#     if idx <= 10:\n",
    "#         m.states[m.mapping[i]] = 0\n",
    "time_signal = m.simulate(int(1e5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "# mean = np.real(np.exp(2 * np.pi * np.complex(0, 1) * time_signal))\n",
    "mean = time_signal.mean(1)\n",
    "filtered = ndimage.gaussian_filter1d(mean, 100) # filter out noise\n",
    "\n",
    "idx  = np.where(filtered < .5)[0]\n",
    "diff = np.ones(mean.shape)\n",
    "diff[idx] = 0\n",
    "switchidx = np.diff(diff)\n",
    "deltas = np.zeros(mean.shape) \n",
    "deltas[np.where(switchidx == 1)[0]] = 1\n",
    "switcher = np.where(abs(switchidx) == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = go.Scatter(y = mean, name = 'original trace')\n",
    "dif = go.Scatter(y = diff, name = 'Delta for switches')\n",
    "f   = go.Scatter(y = filtered, name = 'Gaussian filtered')\n",
    "g    = go.Scatter(y = deltas, name = 'deltas')\n",
    "# h    = go.Scatter(y = tmp, name = 'hallo')\n",
    "fig = go.Figure(data = [line, dif, f, g])\n",
    "fig.update_layout(\\\n",
    "                  title = 'Potts',\\\n",
    "                 xaxis = dict(title = 'time'),\\\n",
    "                 yaxis = dict(title = 'mean magnetization'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 4)\n",
    "p = lambda x, b: 1/(1 + np.exp(-x * beta))\n",
    "betas = np.linspace(.001, 1, 5)\n",
    "beta  = 1/temps\n",
    "\n",
    "lines = []\n",
    "for beta in betas:\n",
    "    line = go.Scatter(x = x, y = p(x, beta),\\\n",
    "                     name = 1/beta)\n",
    "    lines.append(line)\n",
    "fig = go.Figure(data = lines)\n",
    "fig.update_layout(yaxis_title = 'P(s = 1 | x)', xaxis_title = 'x')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a lattice graph, degree does not make a difference to the switching probability. The only difference would be the local energy that a nodes obtains. This depends on the initial conditions and the temporal dynamics over time. Local frustrations occur randomly and this noise is transmitted. The succes of noise transmission would be dependendent on the susceptibility of the nodes which is dependent on the temperature of the system. The higher the temperature, the smaller the beta value, and the more linear the node will transmit noise.  \n",
    "\n",
    "This can be seen by the fact that for lower beta (above) the curve is flatter. Consequently, if a node were to receive arround 0 energy, the flip probability will remain around .5 as the energy input diverges from 0, for higher beta (lower temperature), the node tends to favor one or the other state as the energy diverges from zero. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from jupyter_plotly_dash import JupyterDash\n",
    "import plotly.express as px\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "tmp = []\n",
    "window = 1\n",
    "\n",
    "for i in switcher:\n",
    "    for j in range(-window, window):\n",
    "        tmp.append(i + j)\n",
    "# print(len(switcher), len(tmp))\n",
    "DO = np.array(tmp)\n",
    "app = JupyterDash('SimpleExample')\n",
    "app.layout = html.Div([\n",
    "    html.Div(id = 'app_test'),\\\n",
    "    html.Div([\n",
    "        dcc.Slider(\n",
    "        id   = 'time-slider',\n",
    "        min  = 0,\n",
    "        max  = len(tmp) - 1,\n",
    "        step = 1,\n",
    "        value= 0,\n",
    "        updatemode =  'drag'),\\\n",
    "    dcc.Graph(id = 'animation'),\\\n",
    "    ],\n",
    "    ),\n",
    "    html.Div([\n",
    "\n",
    "    ],\\\n",
    "        style = dict(width = 50, height = 50,\\\n",
    "                      margin = 'auto')),\\\n",
    "    html.Div(id = 'time-slider-output')\n",
    "\n",
    "])\n",
    "\n",
    "from plotly import subplots\n",
    "colors = px.colors.sequential.Magenta\n",
    "idx    = np.linspace(0, len(colors) - 1, len(m.agentStates), dtype = int)\n",
    "# print(idx)\n",
    "colors = [(j / (m.nStates-1), np.array(colors)[i]) for j, i in  enumerate(idx)]\n",
    "\n",
    "hm = go.Heatmap( z = time_signal[0].reshape(n, n), \\\n",
    "                colorscale = colors,\\\n",
    "                zmin = m.agentStates[0],\\\n",
    "                zmax = m.agentStates[-1],\\\n",
    "               )\n",
    "# print(DO)\n",
    "fig = go.Figure(data = hm)\n",
    "fig.update_layout( template = 'plotly_white', \\\n",
    "                 autosize = False, width = 600, height = 600)\n",
    "\n",
    "@app.callback(\n",
    "    Output('animation', 'figure'),\n",
    "    [Input('time-slider', 'value')])\n",
    "def update_output(value):\n",
    "    try:\n",
    "        for d in fig.data:\n",
    "            d.z = time_signal[DO[value]].reshape(n, n)\n",
    "            fig.update_layout(title = f'{DO[value]}')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return fig \n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '../')\n",
    "from Toolbox import infcy\n",
    "d = time_signal[DO[::2]]\n",
    "s = {tuple(i): 1/len(d) for i in d}\n",
    "print(len(s))\n",
    "sim = infcy.Simulator(m)\n",
    "s, c =  sim.forward(s, \\\n",
    "                   repeats = int(1e4),\\\n",
    "                   time_steps  = 5).values()\n",
    "print(type(c), type(s))\n",
    "_, mi = infcy.mutualInformation(c, s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for i in range(m.nNodes):\n",
    "    line = go.Scatter(y = mi[:, i], name = m.rmapping[i])\n",
    "    lines.append(line)\n",
    "fig = go.Figure(data = lines)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(m.graph, with_labels = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens at these transitions? \n",
    "Given that the degree is not important, we wonder what happens at these transitions points. Why does the transition occur? Let's look at two cases\n",
    "\n",
    "Define $T_0$ the time at transition\n",
    "- What distribution does the system go into as we repeat from this state forwards?\n",
    "   - Does it always flip? --> Probably not as it it a probabilistic model. We would find in the limit the transition ratios given by the node update rule.\n",
    "- How big of a block does the system need to have to switch?\n",
    "    - Start with all nodes at state = 1, then look for time of switch to occur.\n",
    "    - Slowly introduce a block with nodes = 0, then look for the time of switch to occur\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tipping points information decomposition\n",
    "- Get a N tipping points\n",
    "- Plot IMI as a function of T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Toolbox import infcy\n",
    "\n",
    "from pyprind import ProgBar\n",
    "def capture_tipping(model, n_samples, N, window_size):\n",
    "#     buffer = model.simulate(window_size * 2)\n",
    "    buffer = m.simulate(n_samples + 2 * window_size)\n",
    "    stores = {}\n",
    "    n = 0\n",
    "    j = 0\n",
    "    pbar = ProgBar(n_samples)\n",
    "    for ni in range(N):\n",
    "#         buffer[:-1] = buffer[1:]\n",
    "        buffer  = m.simulate(n_samples + 2 * window_size)\n",
    "        # assert tipping point\n",
    "        tipps = np.diff(np.sign(buffer.mean(1) - .5))\n",
    "#         print(abs(tipps), buffer.mean(1) - .5)\n",
    "        \n",
    "        idx = np.argwhere(abs(tipps) > 0)\n",
    "        # located:\n",
    "        #    center the buffer\n",
    "        pbar.update(1)\n",
    "        if len(idx):\n",
    "            idx = idx[0]\n",
    "            pbar.update(1)\n",
    "#             print(idx)\n",
    "            for i in idx:\n",
    "                if i <= window_size or i + window_size >= n_samples:\n",
    "                    continue\n",
    "                else:\n",
    "                    state = tuple(buffer[i].tolist())\n",
    "                    stores[state] = stores.get(state, []) + \\\n",
    "                    [buffer[i - window_size: i + window_size, :]]\n",
    "        \n",
    "    return stores\n",
    "                \n",
    "\n",
    "# print(m.simulate(10).mean(1))\n",
    "m.t = 5\n",
    "sim = infcy.Simulator(m)\n",
    "n_samples = int(1e5)\n",
    "window    = 50\n",
    "center    = True\n",
    "tipps = capture_tipping(m, n_samples, 100, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch time \n",
    "   - Start with all nodes at state = 1, then look for time of switch to occur.\n",
    "   - Slowly introduce a block with nodes = 0, then look for the time of switch to occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrials = 50\n",
    "# neighbors = np.arange(8)\n",
    "neighbors = np.array([5, 10, 30])\n",
    "timings = np.zeros((neighbors.size, nTrials))\n",
    "target = '(0, 0)'\n",
    "for idx, nei in enumerate(neighbors):\n",
    "    for trial in range(nTrials):\n",
    "        m.states = 1\n",
    "        for jdx, node in enumerate(nx.shortest_path(m.graph, source = target)):\n",
    "            m.states[m.mapping[node]] = 0\n",
    "            if jdx >= nei:\n",
    "                break\n",
    "            \n",
    "        res      = m.simulate(int(1e5)).mean(1)\n",
    "        filtered = ndimage.gaussian_filter(res, 100)\n",
    "        try:\n",
    "            zdx = np.where(filtered < .5)[0][0]\n",
    "        except:\n",
    "            zdx = np.nan\n",
    "        timings[idx, trial] = zdx\n",
    "    if idx:\n",
    "        print(idx, end = ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "scatter = go.Scatter(x = neighbors, \\\n",
    "                     y = np.nanmean(timings, 1), \\\n",
    "                     error_y = dict(array = np.nanstd(timings, axis = 1)\\\n",
    "                                   )\\\n",
    "                    )\n",
    "fig = go.Figure(data = scatter)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of switching [with correations from original draft]\n",
    "\n",
    "The fraction of nodes in state $+1$ is defined as\n",
    "\n",
    "$$ p^t = \\frac{(N + M^t)}{2N}$$\n",
    "original draft stated\n",
    "\n",
    "$$p^t = \\frac{(N + M^t)}{2}$$ but this is not normalized...\n",
    "\n",
    "with $M^t = \\sum_i^N x_i^t, x_i^t \\in \\{-1, 1\\}$. \n",
    "\n",
    "Next the assumption is made that for any node, this fraction get carried and as such the number of nodes in +1 can be written as\n",
    "\n",
    "$$ E(x_i^t) = p_i^t k_i$$ \n",
    "\n",
    "\n",
    "Therefore the switch probabililty for a node in the kinetic Ising model can be written  down\n",
    "\n",
    "$$ P(x_i^{t+1} = -x_i^t | X^t) = \\frac{ \\exp(-\\beta E(-x_i^t, X^t) }{ \\exp(-\\beta E(-x_i^t, X^t) + \\exp(-\\beta E(x_i^t, x^t)} = \\frac{1}{ 1 + \\exp(\\beta k_i (2 p_i^t - 1))}$$\n",
    "\n",
    "Note that this assumption only works for mean-field, but then the whole idea of having degree is gone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly import graph_objects as go\n",
    "degrees = np.linspace(1, 100, 100, dtype = int)\n",
    "beta = np.linspace(0, 5, 10)\n",
    "fractions = np.linspace(0, 1, 100)\n",
    "\n",
    "x, y, z = np.meshgrid(degrees, beta, fractions)\n",
    "p = 1/ (1 + np.exp(x * (2 * z - 1) / y))\n",
    "print(p.shape)\n",
    "figs = make_subplots(rows = 1,\\\n",
    "                    cols = beta.size, \\\n",
    "            shared_xaxes = True,\\\n",
    "            shared_yaxes = True, \\\n",
    "            x_title = 'Degree',\\\n",
    "            y_title = 'Fraction',\\\n",
    "            horizontal_spacing = .0,\\\n",
    "#             vertical_spacing = 0,\\\n",
    "                )\n",
    "cmin = p.min()\n",
    "cmax = p.max()\n",
    "\n",
    "print(x.shape)\n",
    "for idx in range(beta.size):\n",
    "    hm = go.Heatmap(\\\n",
    "#                     x = x[idx], \n",
    "#                     y = z[idx],\\\n",
    "                    z = p[idx], zmin = cmin, zmax = cmax,\\\n",
    "#                    xaxis = dict(title = 'hello'),\\\n",
    "                   )\n",
    "    figs.add_trace(hm, col = idx + 1, row = 1)\n",
    "figs.show()\n",
    "# p = 1 / np.exp(beta * degre * (2 * fract - 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
